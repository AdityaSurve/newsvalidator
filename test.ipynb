{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_name = input(\"What is the name of the player you want to search for? \")\n",
    "platform = input(\"What platform is the player on? \")\n",
    "type = input(\"What is the type of the player? \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_player_name = player_name.replace(\" \", \"+\")\n",
    "processed_platform = platform.lower()\n",
    "processed_type = type.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid input\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "query = f\"https://www.bcci.tv/search?platform={processed_platform}&type={processed_type}&term={processed_player_name}&content_type=all\"\n",
    "\n",
    "if not processed_player_name or not processed_platform or not processed_type:\n",
    "    print(\"Invalid input\")\n",
    "    exit()\n",
    "else:\n",
    "    try:\n",
    "        response = requests.get(query)\n",
    "        response.raise_for_status()\n",
    "    except requests.RequestException as e:\n",
    "        print(e)\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    data = []\n",
    "\n",
    "    player_data = soup.find('div', class_='lv pc video-section-append-video-here')\n",
    "    if player_data:\n",
    "        player_card = player_data.find_all(\n",
    "            'div', class_='slick-card m-0 lv-bg hoverVideoPlayNow')\n",
    "        for card in player_card:\n",
    "            header_content = card.find('div', class_='bottom')\n",
    "            title_holder = None\n",
    "            title = \"\"\n",
    "            date = \"\"\n",
    "            views = \"\"\n",
    "            if header_content.find('div', class_='text-detail br-b'):\n",
    "                title_holder = header_content.find('div', class_='text-detail br-b')\n",
    "                date_holder = header_content.find('div', class_='text-detail br-b')\n",
    "                date =  date_holder.find('span').text\n",
    "                views_holder = header_content.find(\n",
    "                    'div', class_='tour-overlay-details')\n",
    "                list = views_holder.find('ul')\n",
    "                views = list.find_all('li')[0]\n",
    "                views = views.find('span', class_=\"me-3\").text\n",
    "                views = views.replace(\"&nbsp;\", \"\").replace(\n",
    "                    \"views\", \"\").replace(\"\\n\", \"\").replace(\"\\xa0\", \"\").strip()\n",
    "                if \"k\" in views:\n",
    "                    views = views.replace(\"k\", \"\")\n",
    "                    views = float(views) * 1000\n",
    "                elif \"m\" in views:\n",
    "                    views = views.replace(\"m\", \"\")\n",
    "                    views = float(views) * 1000000\n",
    "            elif header_content.find('div', class_='text-detail pb-0'):\n",
    "                title_holder = header_content.find('div', class_='text-detail pb-0')\n",
    "                date_holder = header_content.find(\n",
    "                    'div', class_='tour-overlay-details')\n",
    "                ul = date_holder.find('ul')\n",
    "                date_ = ul.find_all('li')[0]\n",
    "                date = date_.find('span').text\n",
    "                views = \"-\"\n",
    "            if title_holder:\n",
    "                title = title_holder.find('p').text\n",
    "\n",
    "            data.append({\n",
    "                \"title\": title,\n",
    "                \"date\": date,\n",
    "                \"views\": views,\n",
    "                \"platform\": processed_platform,\n",
    "                \"type\": processed_type,\n",
    "                \"player_name\": player_name,\n",
    "                \"sport\": \"Cricket\"\n",
    "            })\n",
    "        if data:\n",
    "            df = pd.DataFrame(data)\n",
    "            header = ['title', 'date', 'views', 'platform', 'type', 'player_name']\n",
    "            if os.path.isfile('Sports.csv') and os.path.getsize('Sports.csv') > 0:\n",
    "                df.to_csv('Sports.csv', mode='a', header=False, index=False)\n",
    "            else:\n",
    "                df.to_csv('Sports.csv', mode='a', header=header, index=False)\n",
    "    else:\n",
    "        print(\"Player not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "class ICC_Scrapper:\n",
    "    def __init__(self):\n",
    "        self.url = \"https://www.icc-cricket.com/search?\"\n",
    "        self.player_name = None\n",
    "\n",
    "    def get_player_data(self, player_name):\n",
    "        processed_player_name = player_name.replace(\" \", \"%20\")\n",
    "\n",
    "        if not processed_player_name:\n",
    "            return {\"Error\": \"Invalid input\"}\n",
    "\n",
    "        self.player_name = processed_player_name\n",
    "\n",
    "        url = self.url + \"q=\" + self.player_name\n",
    "\n",
    "        response = self.get_data(processed_player_name, url)\n",
    "\n",
    "        if response:\n",
    "            return {\"Response\": response}\n",
    "        else:\n",
    "            return {\"Error\": \"No data found\"}\n",
    "\n",
    "    def get_data(self, player_name, url):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "        except requests.RequestException as e:\n",
    "            return {\"Error\": str(e)}\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        data = []\n",
    "\n",
    "        player_data = soup.find(\n",
    "            'div', class_='my-4 grid grid-cols-1 lg:grid-cols-4 gap-6 lg:gap-x-6 lg:gap-y-10')\n",
    "        if player_data:\n",
    "            cards = player_data.find_all(\n",
    "                'div', class_='h-[541px] relative rounded-lg lg:rounded-[14px] overflow-hidden')\n",
    "            for card in cards:\n",
    "                title = \"\"\n",
    "                date = \"\"\n",
    "                content = \"\"\n",
    "                image_url = \"\"\n",
    "                link = \"\"\n",
    "                link_holder = card.find('a')\n",
    "                link = link_holder['href']\n",
    "                image_holder = link_holder.find_all(\n",
    "                    'div')[0]\n",
    "                image_holder = image_holder.find('picture')\n",
    "                image_holder = image_holder.find('img')\n",
    "                image_url = image_holder['src']\n",
    "                divs = link_holder.find_all('div')\n",
    "                if len(divs) > 1:\n",
    "                    content_holder = divs[1]\n",
    "                    content_holder = content_holder.find('div')\n",
    "                    title_holder = content_holder.find_all('div')[1] if len(\n",
    "                        content_holder.find_all('div')) > 1 else None\n",
    "                    if title_holder:\n",
    "                        title = title_holder.text\n",
    "                    content_holder = content_holder.find(\n",
    "                        'div', class_='text-sm font-bold text-white leading-[1.2] lg:text-lg lg:leading-[1.4] lg:-tracking-[0.72px]')\n",
    "                    if content_holder:\n",
    "                        content = content_holder.text\n",
    "                    date_holder = content_holder.find('time')\n",
    "                    date = date_holder.text if date_holder else \"\"\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                temp_player_name = player_name.replace(\"%20\", \" \")\n",
    "                data.append({\n",
    "                    \"title\": title,\n",
    "                    \"date\": date,\n",
    "                    \"content\": content,\n",
    "                    \"player_name\": temp_player_name,\n",
    "                    \"image_url\": image_url,\n",
    "                    \"link\": link,\n",
    "                    \"sport\": \"Cricket\"\n",
    "                })\n",
    "\n",
    "            if data:\n",
    "                df = pd.DataFrame(data)\n",
    "                header = ['title', 'date', 'content',\n",
    "                          'player_name', 'image_url', 'link', 'sport']\n",
    "                if os.path.isfile('ICC.csv') and os.path.getsize('ICC.csv') > 0:\n",
    "                    df.to_csv('ICC.csv', mode='a', header=False, index=False)\n",
    "                else:\n",
    "                    df.to_csv('ICC.csv', mode='a', header=header, index=False)\n",
    "                return data\n",
    "            else:\n",
    "                return {\"Error\": \"No data found\"}\n",
    "        else:\n",
    "            return {\"Error\": \"No data found\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapper = ICC_Scrapper()\n",
    "\n",
    "# player_name = \"MS Dhoni\"\n",
    "# scrapper.get_player_data(player_name)\n",
    "\n",
    "player_name = \"Sachin Tendulkar\"\n",
    "scrapper.get_player_data(player_name)\n",
    "\n",
    "# player_name = \"Sourav Ganguly\"\n",
    "# scrapper.get_player_data(player_name)\n",
    "\n",
    "# player_name = \"Ravindra Jadeja\"\n",
    "# scrapper.get_player_data(player_name)\n",
    "\n",
    "# player_name = \"Jasprit Bumrah\"\n",
    "# scrapper.get_player_data(player_name)\n",
    "\n",
    "# player_name = \"Mithali Raj\"\n",
    "# scrapper.get_player_data(player_name)\n",
    "\n",
    "# player_name = \"Smriti Mandhana\"\n",
    "# scrapper.get_player_data(player_name)\n",
    "\n",
    "# player_name = \"Jhulan Goswami\"\n",
    "# scrapper.get_player_data(player_name)\n",
    "\n",
    "# player_name = \"Harmanpreet Kaur\"\n",
    "# scrapper.get_player_data(player_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "class FIFA_Scrapper:\n",
    "    def __init__(self):\n",
    "        self.url = \"https://www.fifa.com/en/search?\"\n",
    "        self.player_name = None\n",
    "\n",
    "    def get_player_data(self, player_name):\n",
    "        processed_player_name = player_name.replace(\" \", \"%20\")\n",
    "\n",
    "        if not processed_player_name:\n",
    "            return {\"Error\": \"Invalid input\"}\n",
    "\n",
    "        self.player_name = processed_player_name\n",
    "\n",
    "        url = self.url + \"q=\" + self.player_name\n",
    "\n",
    "        response = self.get_data(processed_player_name, url)\n",
    "\n",
    "        if response:\n",
    "            return {\"Response\": response}\n",
    "        else:\n",
    "            return {\"Error\": \"No data found\"}\n",
    "\n",
    "    def get_data(self, player_name, url):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "        except requests.RequestException as e:\n",
    "            return {\"Error\": str(e)}\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        data = []\n",
    "\n",
    "        player_data = soup.find(\n",
    "            'div', class_='search-results-page_searchResults__pKRI-')\n",
    "        if player_data:\n",
    "            title = \"\"\n",
    "            date = \"\"\n",
    "            content = \"\"\n",
    "            image_url = \"\"\n",
    "            link = \"\"\n",
    "            player_name = \"\"\n",
    "            sport = \"Football\"\n",
    "            cards = player_data.find_all('a')\n",
    "            for card in cards:\n",
    "                link = card['href']\n",
    "                date_holder = card.find(\n",
    "                    'div', class_='search-result-card_details__+1JIM')\n",
    "                date = date_holder.find('span')[1].text\n",
    "                image_holder = card.find(\n",
    "                    'div', class_='search-result-card_imageContainer__NgxbS')\n",
    "                image_holder = image_holder.find(\n",
    "                    'div', class_='image_imgContainer__nDjya')\n",
    "                image_holder = image_holder.find('img')\n",
    "                image_url = image_holder['src']\n",
    "                content_holder = card.find(\n",
    "                    'div', class_='search-result-card_textContainer__82BrL')\n",
    "                title_holder = content_holder.find(\n",
    "                    'div', class_='search-result-card_title__CTmi4')\n",
    "                title = title_holder.text\n",
    "                content_holder = content_holder.find(\n",
    "                    'div', class_='search-result-card_description__KiKIZ search-result-card_smallCardDescription__InIop')\n",
    "                content = content_holder.text\n",
    "                temp_player_name = player_name.replace(\"%20\", \" \")\n",
    "                data.append({\n",
    "                    \"title\": title,\n",
    "                    \"date\": date,\n",
    "                    \"content\": content,\n",
    "                    \"player_name\": temp_player_name,\n",
    "                    \"image_url\": image_url,\n",
    "                    \"link\": link,\n",
    "                    \"sport\": sport\n",
    "                })\n",
    "\n",
    "            if data:\n",
    "                df = pd.DataFrame(data)\n",
    "                header = ['title', 'date', 'content',\n",
    "                          'player_name', 'image_url', 'link', 'sport']\n",
    "                if os.path.isfile('FIFA.csv') and os.path.getsize('FIFA.csv') > 0:\n",
    "                    df.to_csv('FIFA.csv', mode='a', header=False, index=False)\n",
    "                else:\n",
    "                    df.to_csv('FIFA.csv', mode='a', header=header, index=False)\n",
    "                return data\n",
    "            else:\n",
    "                return {\"Error\": \"No data found\"}\n",
    "        else:\n",
    "            return {\"Error\": \"No data found\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Response': {'Error': '401 Client Error: Unauthorized for url: https://www.fifa.com/en/search?q=Lionel%20Messi'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrapper = FIFA_Scrapper()\n",
    "\n",
    "player_name = \"Lionel Messi\"\n",
    "scrapper.get_player_data(player_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
