{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    }
   ],
   "source": [
    "from SportsScrapper import BCCI_Scrapper, ICC_Scrapper, Indian_Athletes_Scrapper\n",
    "import requests\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import pipeline\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.cluster import KMeans\n",
    "from textblob import TextBlob\n",
    "import numpy as np\n",
    "\n",
    "sentiment_pipeline = pipeline('sentiment-analysis')\n",
    "political_influence_model = pipeline(\n",
    "    'text-classification', model='typeform/distilbert-base-uncased-mnli')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "def bert_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt',\n",
    "                       max_length=512, truncation=True, padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def search_official(query, player_type, player_platform, search_type):\n",
    "    data = []\n",
    "    if search_type == 'bcci':\n",
    "        scrapper = BCCI_Scrapper()\n",
    "        response = scrapper.get_player_data(\n",
    "            query, player_platform, player_type)\n",
    "        data.extend(response['Response'])\n",
    "    elif search_type == 'icc':\n",
    "        scrapper = ICC_Scrapper()\n",
    "        response = scrapper.get_player_data(query)\n",
    "        data.extend(response['Response'])\n",
    "    elif search_type == 'indian_athletes':\n",
    "        scrapper = Indian_Athletes_Scrapper()\n",
    "        response = scrapper.get_player_data(query)\n",
    "        data.extend(response['Response'])\n",
    "    else:\n",
    "        scrapper = BCCI_Scrapper()\n",
    "        response = scrapper.get_player_data(\n",
    "            query, player_platform, player_type)\n",
    "        data.extend(response['Response'])\n",
    "        scrapper = ICC_Scrapper()\n",
    "        response = scrapper.get_player_data(query)\n",
    "        data.extend(response['Response'])\n",
    "        scrapper = Indian_Athletes_Scrapper()\n",
    "        response = scrapper.get_player_data(query)\n",
    "        data.extend(response['Response'])\n",
    "    return data\n",
    "\n",
    "def search_unofficial(query):\n",
    "    query = query.lower()\n",
    "    query = query.replace(' ', '-')\n",
    "    url = 'https://newsapi.org/v2/everything?'\n",
    "    parameters = {\n",
    "        'q': query,\n",
    "        'apiKey': '399a3fe0b00b4bbfa2188e79abdc5b8b',\n",
    "        'sources': 'the-times-of-india,the-hindu,hindustan-times,the-indian-express,news18,ndtv,india-today,zee-news,abp-news,india-tv,republic-world,the-quint,the-wire,scroll,the-print',\n",
    "    }\n",
    "    response = requests.get(url, params=parameters)\n",
    "    data = response.json()\n",
    "    return data['articles']\n",
    "\n",
    "\n",
    "def assess_truth(unofficial_data, official_data):\n",
    "    truth_values = []\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    for article in unofficial_data:\n",
    "        unofficial_text = f\"{article['title']} {article['description']}\"\n",
    "        similarity_scores = []\n",
    "\n",
    "        for official_article in official_data:\n",
    "            official_text = f\"{official_article['title']} {official_article['player_name']}\"\n",
    "            vectors = vectorizer.fit_transform(\n",
    "                [official_text, unofficial_text])\n",
    "            similarity = cosine_similarity(vectors[0:1], vectors[1:2])[0][0]\n",
    "            similarity_scores.append(similarity)\n",
    "        truth_value = max(similarity_scores)\n",
    "        if article['source']['name'] in ['The Times of India', 'The Hindu', 'Hindustan Times', 'The Indian Express', 'News18', 'NDTV', 'India Today', 'Zee News', 'ABP News', 'India TV', 'Republic World', 'The Quint', 'The Wire', 'Scroll', 'The Print']:\n",
    "            truth_value = min(truth_value + 0.4, 1)\n",
    "        truth_values.append(truth_value)\n",
    "    return truth_values\n",
    "\n",
    "\n",
    "def detect_influence(article):\n",
    "    content = article['content']\n",
    "    sentiment_result = sentiment_pipeline(content)\n",
    "    emotional_influence = sentiment_result[0]['label'] in [\n",
    "        'NEGATIVE', 'POSITIVE']\n",
    "    political_result = political_influence_model(content)\n",
    "    political_influence = any(\n",
    "        label['label'] == 'POLITICS' and label['score'] > 0.5 for label in political_result)\n",
    "    return political_influence, emotional_influence\n",
    "\n",
    "\n",
    "def cluster_articles(articles):\n",
    "    contents = [article['content'] for article in articles]\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    X = vectorizer.fit_transform(contents)\n",
    "\n",
    "    true_k = 5\n",
    "    model = KMeans(n_clusters=true_k, random_state=42)\n",
    "    model.fit(X)\n",
    "\n",
    "    labels = model.labels_\n",
    "    cluster_dict = {i: [] for i in range(true_k)}\n",
    "    for idx, label in enumerate(labels):\n",
    "        cluster_dict[label].append(articles[idx])\n",
    "\n",
    "    return cluster_dict\n",
    "\n",
    "\n",
    "def sentiment_analysis(article):\n",
    "    analysis = TextBlob(article['content'])\n",
    "    polarity = analysis.sentiment.polarity\n",
    "    subjectivity = analysis.sentiment.subjectivity\n",
    "    if polarity > 0:\n",
    "        sentiment = 'Positive, which means the text is expressing positive emotions or opinions'\n",
    "    elif polarity < 0:\n",
    "        sentiment = 'Negative, which means the text is expressing negative emotions or opinions'\n",
    "    else:\n",
    "        sentiment = 'Neutral, which means the text is neither positive nor negative'\n",
    "    if subjectivity >= 0.5:\n",
    "        objectivity = 'Subjective, which means the text is based on opinions or beliefs'\n",
    "    else:\n",
    "        objectivity = 'Objective, which means the text is based on facts or evidence'\n",
    "\n",
    "    return {\n",
    "        'polarity': polarity,\n",
    "        'polarity_label': sentiment,\n",
    "        'subjectivity': subjectivity,\n",
    "        'subjectivity_label': objectivity\n",
    "    }\n",
    "\n",
    "\n",
    "def relevance_score(article, query):\n",
    "    title = article['title']\n",
    "    description = article['description']\n",
    "    content = article['content']\n",
    "    combined_text = f\"{title} {description} {content}\"\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectors = vectorizer.fit_transform([query, combined_text])\n",
    "    tfidf_similarity = cosine_similarity(vectors[0:1], vectors[1:2])[0][0]\n",
    "    query_embedding = bert_embedding(query)\n",
    "    text_embedding = bert_embedding(combined_text)\n",
    "    bert_similarity = cosine_similarity(query_embedding, text_embedding)[0][0]\n",
    "    return {\n",
    "        'tfidf_similarity': tfidf_similarity,\n",
    "        'bert_similarity': bert_similarity\n",
    "    }\n",
    "\n",
    "\n",
    "def search(query, player_type, player_platform, type):\n",
    "    official_data = search_official(query, player_type, player_platform, type)\n",
    "    unofficial_data = search_unofficial(query)\n",
    "\n",
    "    truth_values = assess_truth(unofficial_data, official_data)\n",
    "    influences = [detect_influence(article) for article in unofficial_data]\n",
    "    clustered_articles = cluster_articles(unofficial_data)\n",
    "    sentiments = [sentiment_analysis(article) for article in unofficial_data]\n",
    "    relevance_scores = [relevance_score(article, query)\n",
    "                        for article in unofficial_data]\n",
    "\n",
    "    for i, article in enumerate(unofficial_data):\n",
    "        article['truth_value'] = truth_values[i]\n",
    "        article['political_influence'], article['emotional_influence'] = influences[i]\n",
    "        article['sentiment_polarity'] = sentiments[i]['polarity']\n",
    "        article['sentiment_polarity_label'] = sentiments[i]['polarity_label']\n",
    "        article['sentiment_subjectivity'] = sentiments[i]['subjectivity']\n",
    "        article['sentiment_subjectivity_label'] = sentiments[i]['subjectivity_label']\n",
    "        article['relevance_score_tfidf'] = relevance_scores[i]['tfidf_similarity']\n",
    "        article['relevance_score_bert'] = relevance_scores[i]['bert_similarity']\n",
    "\n",
    "    unofficial_data = sorted(unofficial_data, key=lambda x: x['truth_value'], reverse=True)\n",
    "\n",
    "    result = {\n",
    "        'official_data': official_data,\n",
    "        'unofficial_data': unofficial_data,\n",
    "        'clusters': clustered_articles,\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "class CustomEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.float32):\n",
    "            return float(obj)\n",
    "        return super().default(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTPSConnectionPool(host='indianathletics.in', port=443): Max retries exceeded with url: /?s=virat+kohli (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001EF2707ACD0>, 'Connection to indianathletics.in timed out. (connect timeout=None)'))\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'response' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Aditya\\Documents\\Aditya\\Research\\NewsPhishing\\test.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Aditya/Documents/Aditya/Research/NewsPhishing/test.ipynb#X60sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m result \u001b[39m=\u001b[39m search(\u001b[39m'\u001b[39;49m\u001b[39mvirat kohli\u001b[39;49m\u001b[39m'\u001b[39;49m , \u001b[39m'\u001b[39;49m\u001b[39mmen\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39minternational\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mall\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Aditya/Documents/Aditya/Research/NewsPhishing/test.ipynb#X60sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(json\u001b[39m.\u001b[39mdumps(result, \u001b[39mcls\u001b[39m\u001b[39m=\u001b[39mCustomEncoder, indent\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m))\n",
      "\u001b[1;32mc:\\Users\\Aditya\\Documents\\Aditya\\Research\\NewsPhishing\\test.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Aditya/Documents/Aditya/Research/NewsPhishing/test.ipynb#X60sZmlsZQ%3D%3D?line=155'>156</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msearch\u001b[39m(query, player_type, player_platform, \u001b[39mtype\u001b[39m):\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Aditya/Documents/Aditya/Research/NewsPhishing/test.ipynb#X60sZmlsZQ%3D%3D?line=156'>157</a>\u001b[0m     official_data \u001b[39m=\u001b[39m search_official(query, player_type, player_platform, \u001b[39mtype\u001b[39;49m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Aditya/Documents/Aditya/Research/NewsPhishing/test.ipynb#X60sZmlsZQ%3D%3D?line=157'>158</a>\u001b[0m     unofficial_data \u001b[39m=\u001b[39m search_unofficial(query)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Aditya/Documents/Aditya/Research/NewsPhishing/test.ipynb#X60sZmlsZQ%3D%3D?line=159'>160</a>\u001b[0m     truth_values \u001b[39m=\u001b[39m assess_truth(unofficial_data, official_data)\n",
      "\u001b[1;32mc:\\Users\\Aditya\\Documents\\Aditya\\Research\\NewsPhishing\\test.ipynb Cell 3\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Aditya/Documents/Aditya/Research/NewsPhishing/test.ipynb#X60sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     data\u001b[39m.\u001b[39mextend(response[\u001b[39m'\u001b[39m\u001b[39mResponse\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Aditya/Documents/Aditya/Research/NewsPhishing/test.ipynb#X60sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     scrapper \u001b[39m=\u001b[39m Indian_Athletes_Scrapper()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Aditya/Documents/Aditya/Research/NewsPhishing/test.ipynb#X60sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     response \u001b[39m=\u001b[39m scrapper\u001b[39m.\u001b[39;49mget_player_data(query)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Aditya/Documents/Aditya/Research/NewsPhishing/test.ipynb#X60sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     data\u001b[39m.\u001b[39mextend(response[\u001b[39m'\u001b[39m\u001b[39mResponse\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Aditya/Documents/Aditya/Research/NewsPhishing/test.ipynb#X60sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\Aditya\\Documents\\Aditya\\Research\\NewsPhishing\\SportsScrapper.py:136\u001b[0m, in \u001b[0;36mIndian_Athletes_Scrapper.get_player_data\u001b[1;34m(self, player_name)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplayer_name \u001b[39m=\u001b[39m processed_player_name\n\u001b[0;32m    134\u001b[0m url \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39murl \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39ms=\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplayer_name\n\u001b[1;32m--> 136\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_data(processed_player_name, url)\n\u001b[0;32m    138\u001b[0m \u001b[39mif\u001b[39;00m response:\n\u001b[0;32m    139\u001b[0m     \u001b[39mreturn\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mResponse\u001b[39m\u001b[39m\"\u001b[39m: response}\n",
      "File \u001b[1;32mc:\\Users\\Aditya\\Documents\\Aditya\\Research\\NewsPhishing\\SportsScrapper.py:150\u001b[0m, in \u001b[0;36mIndian_Athletes_Scrapper.get_data\u001b[1;34m(self, player_name, url)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mRequestException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    148\u001b[0m     \u001b[39mprint\u001b[39m(e)\n\u001b[1;32m--> 150\u001b[0m soup \u001b[39m=\u001b[39m BeautifulSoup(response\u001b[39m.\u001b[39mtext, \u001b[39m'\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    151\u001b[0m data \u001b[39m=\u001b[39m []\n\u001b[0;32m    153\u001b[0m player_data \u001b[39m=\u001b[39m soup\u001b[39m.\u001b[39mfind(\u001b[39m'\u001b[39m\u001b[39mol\u001b[39m\u001b[39m'\u001b[39m, class_\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msearch_res\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'response' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "result = search('virat kohli' , 'men', 'international', 'bcci')\n",
    "print(json.dumps(result, cls=CustomEncoder, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
